# -*- coding: utf-8 -*-
"""Salinan dari Deteksi Anomali SMOTE-TOMEK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dsbbIhNzdKDkYBBb3Isx6NMg49isTqSS

# Import Library
"""

import pandas as pd
import numpy as np
import os
import joblib
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns

"""# Data Preparation"""

# Data load
try:
    df_train = pd.read_csv('dataset_latihan.csv')
    df_test = pd.read_csv('dataset_tes_holdout.csv')
    print("Dataset Latihan dan Tes Holdout berhasil dimuat.")
except FileNotFoundError:
    print("ERROR: Pastikan file 'dataset_latihan.csv' dan 'dataset_tes_holdout.csv' ada.")
    exit()

# Lakukan pengecekan dan feature engineering pada kedua dataframe
for name, df in [('Latihan', df_train), ('Tes Holdout', df_test)]:
    print(f"\n--- Memproses Dataset {name} ---")

    # Cek Missing Value
    if df.isnull().sum().sum() == 0:
        print("Tidak ada missing value.")
    else:
        print(f"Ditemukan missing value, harap ditangani.")
        print(df.isnull().sum()[df.isnull().sum() > 0])

    # Cek Duplikat
    duplicate_count = df.duplicated().sum()
    print(f"Jumlah baris duplikat: {duplicate_count}")
    if duplicate_count > 0:
        df.drop_duplicates(inplace=True)
        print("Baris duplikat telah dihapus.")

    # Feature Engineering
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['hour_of_day'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek

print("\nFeature engineering selesai untuk kedua dataset.")

"""# Pemrosesan Data"""

# Definisikan fitur (X) dan target (y)
features_to_drop = ['transaction_id', 'sender_account', 'receiver_account', 'timestamp', 'is_synthetic', 'anomaly_type']

X_train = df_train.drop(columns=['is_fraud'] + features_to_drop)
y_train = df_train['is_fraud']

X_test = df_test.drop(columns=['is_fraud'] + features_to_drop)
y_test = df_test['is_fraud']

# Identifikasi kolom numerik dan kategorikal
numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()
categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()

#  PRAPEMROSESAN DATA
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'
)

# fit HANYA pada data LATIHAN
print("\nMelatih preprocessor HANYA pada data latihan...")
preprocessor.fit(X_train)

# Terapkan preprocessor ke kedua set
X_train_processed = preprocessor.transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print(f"Bentuk data training setelah diproses: {X_train_processed.shape}")
print(f"Bentuk data testing setelah diproses: {X_test_processed.shape}")

"""# Modelling"""

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_processed.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
model.summary()

# Hitung class weight dari data latihan
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))
print(f"\nClass Weights yang digunakan: {class_weight_dict}")

print("\nMemulai pelatihan model...")
history = model.fit(
    X_train_processed, y_train,
    epochs=30,
    batch_size=32,
    validation_data=(X_test_processed, y_test),
    class_weight=class_weight_dict,
    verbose=1
)
print("Pelatihan model selesai.")

"""# Evaluasi"""

# --- Fungsi untuk plot grafik ---
def plot_training_history(history):
    fig, ax = plt.subplots(1, 2, figsize=(16, 6))
    ax[0].plot(history.history['loss'], label='Training Loss')
    ax[0].plot(history.history['val_loss'], label='Validation Loss (Holdout)')
    ax[0].set_title('Loss Model'); ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].legend(); ax[0].grid(True)
    ax[1].plot(history.history['auc'], label='Training AUC')
    ax[1].plot(history.history['val_auc'], label='Validation AUC (Holdout)')
    ax[1].set_title('AUC Model'); ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('AUC'); ax[1].legend(); ax[1].grid(True)
    plt.show()

print("\n--- Grafik Performa Pelatihan (diuji pada Holdout Set) ---")
plot_training_history(history)

# Evaluasi final pada holdout set
print("\n--- HASIL EVALUASI FINAL PADA HOLDEM SET ---")
y_pred_proba = model.predict(X_test_processed).ravel()
y_pred = (y_pred_proba > 0.5).astype(int)

print("\nLaporan Klasifikasi (Holdout Set):")
print(classification_report(y_test, y_pred))

roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"ROC-AUC Score (Holdout Set): {roc_auc:.4f}")

print("\nConfusion Matrix (Holdout Set):")
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])
plt.xlabel('Prediksi'); plt.ylabel('Aktual'); plt.title('Confusion Matrix (Holdout Set)')
plt.show()

"""# Saved Model"""

print("\nMenyimpan model dan preprocessor...")
model_dir = 'saved_models'
os.makedirs(model_dir, exist_ok=True)

# a. Simpan dalam format SavedModel (rekomendasi TensorFlow untuk deployment)
# Gunakan model.export() untuk SavedModel format
model.export(os.path.join(model_dir, 'model_savedmodel'))
print(f"Model disimpan dalam format SavedModel di direktori '{model_dir}/model_savedmodel'")

# b. Simpan dalam format Keras (.keras) - modern (untuk Keras/TensorFlow)
# Gunakan model.save() dengan ekstensi .keras
model.save(os.path.join(model_dir, 'model.keras'))
print(f"Model disimpan dalam format Keras di '{model_dir}/model.keras'")

# c. Simpan dalam format HDF5 (.h5) - legacy (untuk kompatibilitas)
# Gunakan model.save() dengan ekstensi .h5
model.save(os.path.join(model_dir, 'model.h5'))
print(f"Model disimpan dalam format H5 di '{model_dir}/model.h5'")

# d. Simpan preprocessor menggunakan joblib
preprocessor_path = os.path.join(model_dir, 'preprocessor.joblib')
joblib.dump(preprocessor, preprocessor_path)
print(f"Preprocessor disimpan di '{preprocessor_path}'")

# %%

"""# Contoh Inferensi"""

print("\n--- Contoh Proses Inferensi ---")

# Muat kembali model dan preprocessor
# Muat model dari format .keras
model_path_keras = os.path.join(model_dir, 'model.keras')
loaded_model = tf.keras.models.load_model(model_path_keras)
loaded_preprocessor = joblib.load(preprocessor_path)
print("Model dan preprocessor berhasil dimuat kembali.")

# Buat contoh data baru (dengan aturan anomali dari holdout set)
new_data_holdout = pd.DataFrame({
    'amount': [8000000.0],
    'transaction_type': ['withdrawal'],
    'merchant_category': ['travel_agency'],
    'location': ['Nairobi'], # <-- Lokasi dari aturan holdout
    'hour_of_day': [14],
    'day_of_week': [2]
})

print("\nData baru untuk diprediksi (pola holdout):")
print(new_data_holdout)

# Terapkan preprocessor
new_data_processed = loaded_preprocessor.transform(new_data_holdout)

# Lakukan prediksi
prediction_proba = loaded_model.predict(new_data_processed)
prediction = (prediction_proba > 0.5).astype(int)

print(f"\nHasil Prediksi (Probabilitas): {prediction_proba[0][0]:.4f}")
if prediction[0][0] == 1:
    print("Kesimpulan: Transaksi terdeteksi sebagai ANOMALI (FRAUD)")
else:
    print("Kesimpulan: Transaksi terdeteksi sebagai NORMAL")

# Contoh kedua
new_data = pd.DataFrame({
    'amount': [250000.0],
    'transaction_type': ['withdrawal'],
    'merchant_category': ['groceries'],
    'location': ['Bandung'],
    'hour_of_day': [14],
    'day_of_week': [5]
})

print("\nData baru untuk diprediksi:")
print(new_data)

# --- 3. LAKUKAN PREDIKSI ---
# Terapkan preprocessor yang sudah dilatih ke data baru
new_data_processed = loaded_preprocessor.transform(new_data)

# Lakukan prediksi
prediction_proba = loaded_model.predict(new_data_processed)
prediction = (prediction_proba > 0.5).astype(int) # Menggunakan threshold 0.5

# --- 4. TAMPILKAN HASIL ---
print(f"\nHasil Prediksi (Probabilitas Fraud): {prediction_proba[0][0]:.4f}")
if prediction[0][0] == 1:
    print("Kesimpulan: Transaksi terdeteksi sebagai ANOMALI (FRAUD)")
else:
    print("Kesimpulan: Transaksi terdeteksi sebagai NORMAL")

# Jalankan ini di Google Colab Anda
!pip freeze > requirements.txt


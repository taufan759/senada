# -*- coding: utf-8 -*-
"""Salinan dari RekomendasiInvestasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dm7yF1sD_VSL_52fBcez_4Uwfvy8uQO0
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
import os

# Memastikan output konsisten
np.random.seed(42)
tf.random.set_seed(42)

print("Langkah 1: Memuat Data...")
try:
    df = pd.read_csv('dataset_investasi_large2.csv')
    print(f"Data berhasil dimuat. Jumlah baris awal: {len(df)}")
except FileNotFoundError:
    print("\nERROR: File 'dataset_investasi_large2.csv' tidak ditemukan.")
    print("Pastikan file CSV berada di folder yang sama dengan script ini.")
    exit()

df.head()

print("[INFO] Mengecek missing values...")
if df.isnull().sum().sum() > 0:
    print(f"Ditemukan missing values. Jumlahnya:\n{df.isnull().sum()[df.isnull().sum() > 0]}")
    # Untuk contoh ini, kita pilih strategi menghapus baris yang mengandung null
    # Untuk kasus riil, pertimbangkan strategi imputasi (mengisi nilai)
    df.dropna(inplace=True)
    print("-> Baris dengan missing values telah dihapus.")
else:
    print("-> Tidak ada missing values ditemukan.")

print("\n[INFO] Mengecek data duplikat...")
duplicate_count = df.duplicated().sum()
if duplicate_count > 0:
    print(f"-> Ditemukan {duplicate_count} baris data duplikat. Menghapus...")
    df.drop_duplicates(inplace=True)
else:
    print("-> Tidak ada data duplikat ditemukan.")

# Reset index setelah cleaning agar urutannya kembali normal
df.reset_index(drop=True, inplace=True)
print(f"\nData Cleaning Selesai. Jumlah baris setelah cleaning: {len(df)}")

print("\nLangkah 2: Pra-pemrosesan Data...")

# Pisahkan fitur dan target
X = df.drop('rekomendasi', axis=1)
y = df['rekomendasi']

# Identifikasi fitur yang tidak relevan untuk training
features_to_drop = ['user_id', 'produk_id', 'nama_produk']
X = X.drop(columns=features_to_drop)

# Identifikasi kolom numerik dan kategorikal
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(include='object').columns.tolist()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Buat pipeline preprocessing
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough'
)

# Terapkan preprocessor
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Simpan preprocessor
joblib.dump(preprocessor, 'investment_preprocessor.joblib')
print("Preprocessor berhasil disimpan sebagai 'investment_preprocessor.joblib'")

print("\nLangkah 3: Membangun Model TensorFlow...")

input_shape = X_train_processed.shape[1]
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(input_shape,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
model.summary()

print("\nLangkah 4: Pelatihan Model...")

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(
    X_train_processed,
    y_train,
    epochs=100,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

print("\nLangkah 5: Evaluasi Model...")
def plot_history(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Plot Grafik Loss
    ax1.plot(history.history['loss'], label='Training Loss')
    ax1.plot(history.history['val_loss'], label='Validation Loss')
    ax1.set_title('Grafik Loss Model')
    ax1.set_ylabel('Loss')
    ax1.set_xlabel('Epoch')
    ax1.legend(loc='upper right')

    # Plot Grafik Akurasi
    ax2.plot(history.history['accuracy'], label='Training Accuracy')
    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')
    ax2.set_title('Grafik Akurasi Model')
    ax2.set_ylabel('Akurasi')
    ax2.set_xlabel('Epoch')
    ax2.legend(loc='lower right')

    plt.show()

plot_history(history)

loss, accuracy = model.evaluate(X_test_processed, y_test, verbose=0)
print(f"\nAkurasi pada data test: {accuracy:.4f}")
print(f"Loss pada data test: {loss:.4f}")

y_pred_proba = model.predict(X_test_processed)
y_pred = (y_pred_proba > 0.5).astype("int32")

print("\nLaporan Klasifikasi:")
print(classification_report(y_test, y_pred, target_names=['Tidak Direkomendasikan', 'Direkomendasikan']))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Tidak', 'Ya'], yticklabels=['Tidak', 'Ya'])
plt.xlabel('Prediksi Rekomendasi')
plt.ylabel('Rekomendasi Sebenarnya')
plt.title('Confusion Matrix')
plt.show()

print("\nLangkah 6: Menyimpan Model dalam 3 Format...")

# Format 1: .keras (Modern & Direkomendasikan)
model.save('investment_recommendation_model.keras')
print("-> Model berhasil disimpan sebagai 'investment_recommendation_model.keras'")

# Format 2: .h5 (Legacy HDF5)
model.save('investment_recommendation_model.h5')
print("-> Model berhasil disimpan sebagai 'investment_recommendation_model.h5'")

# Format 3: SavedModel (Direktori untuk Deployment)
# Gunakan model.export() untuk menyimpan dalam format SavedModel
model.export('investment_recommendation_saved_model')
print("-> Model berhasil disimpan sebagai direktori 'investment_recommendation_saved_model'")

print("\nLangkah 7: Contoh Inferensi pada Data Baru...")

# Muat kembali preprocessor dan model (format .keras sebagai contoh)
loaded_preprocessor = joblib.load('investment_preprocessor.joblib')
loaded_model = tf.keras.models.load_model('investment_recommendation_model.keras')
print("Preprocessor dan Model (.keras) berhasil dimuat untuk inferensi.")

# Buat contoh data baru
new_data = pd.DataFrame({
    'usia': [30],
    'profil_risiko': ['Agresif'],
    'pendapatan_bulanan_juta': [25],
    'tingkat_pengetahuan': ['Menengah'],
    'status_pernikahan': ['Menikah'],
    'jumlah_tanggungan': [1],
    'tujuan_keuangan': ['Dana Pensiun'],
    'jangka_waktu_thn': [25.0],
    'target_dana_juta': [1500],
    'jenis_produk': ['Reksa Dana Indeks Saham'],
    'tingkat_risiko_skor': [8],
    'potensi_return_tahunan_persen': [18.0],
    'likuiditas': ['Sedang'],
    'minimum_investasi_rp': [100000],
})

# Preprocess data baru
new_data_processed = loaded_preprocessor.transform(new_data)

# Lakukan prediksi
predictions_proba = loaded_model.predict(new_data_processed)
predictions = (predictions_proba > 0.5).astype("int32")

# Tampilkan hasil
print("\n--- Hasil Rekomendasi ---")
for i in range(len(new_data)):
    product_name = new_data['jenis_produk'].iloc[i]
    probability = predictions_proba[i][0]
    result = "COCOK" if predictions[i][0] == 1 else "TIDAK COCOK"

    print(f"Produk: {product_name}")
    print(f"  -> Probabilitas Kecocokan: {probability:.2%}")
    print(f"  -> Rekomendasi: **{result}**\n")

!pip freeze > requirements.txt